# ServiceMonitor for Prometheus to scrape metrics
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: llm-router
  namespace: quantumlayer
  labels:
    app: llm-router
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: llm-router
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
    scheme: http
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_name]
      targetLabel: pod
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: node
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: agent-orchestrator
  namespace: quantumlayer
  labels:
    app: agent-orchestrator
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: agent-orchestrator
  endpoints:
  - port: http
    interval: 30s
    path: /metrics
---
# PrometheusRule for alerting
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: quantumlayer-alerts
  namespace: quantumlayer
  labels:
    prometheus: kube-prometheus
spec:
  groups:
  - name: quantumlayer.rules
    interval: 30s
    rules:
    - alert: HighErrorRate
      expr: |
        sum(rate(http_requests_total{status=~"5.."}[5m])) BY (service)
        /
        sum(rate(http_requests_total[5m])) BY (service)
        > 0.05
      for: 5m
      labels:
        severity: critical
        team: platform
      annotations:
        summary: "High error rate on {{ $labels.service }}"
        description: "{{ $labels.service }} has error rate of {{ $value | humanizePercentage }}"
    
    - alert: HighLatency
      expr: |
        histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) BY (service, le))
        > 1
      for: 5m
      labels:
        severity: warning
        team: platform
      annotations:
        summary: "High latency on {{ $labels.service }}"
        description: "99th percentile latency is {{ $value }}s"
    
    - alert: PodCrashLooping
      expr: |
        rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: critical
        team: platform
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
        description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has restarted {{ $value }} times in the last 15 minutes"
    
    - alert: HighMemoryUsage
      expr: |
        container_memory_working_set_bytes{namespace="quantumlayer"}
        / container_spec_memory_limit_bytes{namespace="quantumlayer"}
        > 0.9
      for: 5m
      labels:
        severity: warning
        team: platform
      annotations:
        summary: "High memory usage in {{ $labels.pod }}"
        description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} memory usage is {{ $value | humanizePercentage }}"
    
    - alert: DatabaseConnectionPoolExhausted
      expr: |
        pg_stat_database_numbackends{datname="quantumlayer"}
        / pg_settings_max_connections
        > 0.8
      for: 5m
      labels:
        severity: critical
        team: database
      annotations:
        summary: "Database connection pool nearly exhausted"
        description: "PostgreSQL connection pool is {{ $value | humanizePercentage }} full"
---
# Grafana Dashboard ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: quantumlayer-dashboard
  namespace: quantumlayer
  labels:
    grafana_dashboard: "1"
data:
  quantumlayer-dashboard.json: |
    {
      "dashboard": {
        "title": "QuantumLayer Platform",
        "panels": [
          {
            "title": "Request Rate",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total[5m])) by (service)"
              }
            ]
          },
          {
            "title": "Error Rate",
            "targets": [
              {
                "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) by (service)"
              }
            ]
          },
          {
            "title": "Latency P99",
            "targets": [
              {
                "expr": "histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le))"
              }
            ]
          },
          {
            "title": "LLM Requests",
            "targets": [
              {
                "expr": "sum(rate(llm_requests_total[5m])) by (provider)"
              }
            ]
          },
          {
            "title": "Agent Tasks",
            "targets": [
              {
                "expr": "sum(rate(agent_tasks_total[5m])) by (agent_type, status)"
              }
            ]
          }
        ]
      }
    }